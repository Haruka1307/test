import torch
import torch.nn as nn
import time

def test_8gpu_performance():
    # æ£€æŸ¥GPUæ•°é‡
    num_gpus = torch.cuda.device_count()
    assert num_gpus >= 8, f"éœ€è¦è‡³å°‘8å¼ GPUï¼Œå½“å‰åªæœ‰ {num_gpus} å¼ "
    print(f"âœ… æ£€æµ‹åˆ° {num_gpus} å¼ GPU: {[torch.cuda.get_device_name(i) for i in range(num_gpus)]}")

    # è®¾ç½®æ¯å¼ GPUçš„æµ‹è¯•æ•°æ®ï¼ˆæ˜¾å­˜å ç”¨çº¦10GBï¼‰
    batch_size = 1024
    dim = 8192  # çŸ©é˜µå¤§å° (dim x dim)
    dtype = torch.float32

    # åœ¨æ¯å¼ GPUä¸Šåˆ›å»ºéšæœºçŸ©é˜µ
    inputs = [torch.randn(batch_size, dim, dim, dtype=dtype).to(f'cuda:{i}') for i in range(num_gpus)]
    
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
    model = nn.Linear(dim, dim).cuda()
    model = nn.DataParallel(model, device_ids=range(num_gpus))  # æ•°æ®å¹¶è¡Œ

    # é¢„çƒ­GPUï¼ˆé¿å…é¦–æ¬¡è¿è¡Œé€Ÿåº¦åå·®ï¼‰
    for _ in range(3):
        _ = model(inputs[0])

    # æ­£å¼æµ‹è¯•
    start_time = time.time()
    for i in range(10):  # è¿è¡Œ10æ¬¡è¿­ä»£
        outputs = model(inputs[i % num_gpus])  # è½®è¯¢ä½¿ç”¨ä¸åŒGPUçš„æ•°æ®
    elapsed_time = time.time() - start_time

    # è¾“å‡ºç»“æœ
    print(f"\nâ±ï¸  æ€»è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")
    print(f"ğŸš€ å¹³å‡æ¯æ¬¡è¿­ä»£æ—¶é—´: {elapsed_time / 10:.4f}ç§’")
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼è¿è¡Œ `nvidia-smi` æŸ¥çœ‹æ˜¾å­˜å ç”¨ã€‚")

if __name__ == "__main__":
    test_8gpu_performance()